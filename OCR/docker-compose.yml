version: '3.8'

services:
  vllm-ocr:
    image: vllm/vllm-openai:latest
    container_name: vllm-ocr
    runtime: nvidia
    environment:
      - CUDA_VISIBLE_DEVICES=${CUDA_VISIBLE_DEVICES:-1}
      - MODEL=${MODEL:-rednote-hilab/dots.ocr}
      - GPU_MEMORY_UTILIZATION=${GPU_MEMORY_UTILIZATION:-0.60}
      - PORT=${PORT:-30010}
      # Fix CUDA assertion bug với multi-modal inputs
      - DISABLE_PREFIX_CACHING=${DISABLE_PREFIX_CACHING:-true}
      - MAX_BATCHED_TOKENS=${MAX_BATCHED_TOKENS:-8192}
    command:
      - "vllm"
      - "serve"
      - "${MODEL:-rednote-hilab/dots.ocr}"
      - "--trust-remote-code"
      - "--async-scheduling"
      - "--gpu-memory-utilization"
      - "${GPU_MEMORY_UTILIZATION:-0.60}"
      - "--host"
      - "0.0.0.0"
      - "--port"
      - "${PORT:-30010}"
      - "--no-enable-prefix-caching"
      - "--max-num-batched-tokens"
      - "${MAX_BATCHED_TOKENS:-8192}"
      - "--enforce-eager"
    ports:
      - "${PORT:-30010}:${PORT:-30010}"
    volumes:
      - ./logs:/app/logs
      - ~/.cache/huggingface:/root/.cache/huggingface  # Cache model weights để không download lại
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              capabilities: [gpu]
    restart: unless-stopped
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "3"
    healthcheck:
      test: ["CMD-SHELL", "curl -f http://localhost:30010/v1/models || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 120s  # Đợi 2 phút để model load xong (thời gian model cần để load)

