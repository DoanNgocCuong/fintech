import os
import glob
import base64
import re
from pdf2image import convert_from_path
from openai import OpenAI
import logging
from markdownify import markdownify
import time
import multiprocessing
from typing import Optional
from utils_count import compare_page_counts

# Import utils t·ª´ utils_parallel_num_worker.py
from utils_parallel_num_worker import (
    process_images_parallel
)

# C·∫•u h√¨nh logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    datefmt='%Y-%m-%d %H:%M:%S'
)

logger = logging.getLogger(__name__)

PDF = "/home/ubuntu/fintech/OCR/data/Ng√†nh B·∫£o hi·ªÉm/BIC/2014/Bao_cao_tai_chinh/BIC_2014_1_4_1.pdf"
PDF = "/home/ubuntu/fintech/BaoCaoTaiChinh/OCR/data/test/33_pages_test.pdf"
OUT_DIR = "/home/ubuntu/fintech/BaoCaoTaiChinh/OCR/data/out_images"
MODEL = "rednote-hilab/dots.ocr"
API = "http://103.253.20.30:30010/v1"
OUT_MD = "data/33_pages_test.md"

# Config cho parallel processing
PDF_CONVERT_THREADS = multiprocessing.cpu_count()  # S·ªë cores ƒë·ªÉ convert PDF
# OCR_MAX_WORKERS ph·∫£i ‚â§ server max_num_seqs (hi·ªán t·∫°i server = 8)
# N√™n set = 8 ƒë·ªÉ t·∫≠n d·ª•ng t·ªëi ƒëa server capacity
OCR_MAX_WORKERS = 20  # S·ªë workers cho OCR parallel (ƒë·ªìng b·ªô v·ªõi server max-num-seqs=8)

# ============================================================================
# OPTIMIZE 1: PDF -> Images v·ªõi parallel conversion
# ============================================================================
def pdf2listimages(pdf_path, out_dir, thread_count=None):
    """
    Convert PDF -> Images v·ªõi parallel processing
    ƒê·∫∑t t√™n file ·∫£nh theo format: t√™n_file_pdf-1.png, t√™n_file_pdf-2.png, ...
    """
    os.makedirs(out_dir, exist_ok=True)
    
    if thread_count is None:
        thread_count = PDF_CONVERT_THREADS
    
    # L·∫•y t√™n file PDF (kh√¥ng c√≥ ph·∫ßn m·ªü r·ªông) ƒë·ªÉ ƒë·∫∑t t√™n cho ·∫£nh
    pdf_basename = os.path.splitext(os.path.basename(pdf_path))[0]
    
    logger.info(f"üîÑ Converting PDF to images with {thread_count} threads...")
    convert_start = time.time()
    
    # Convert PDF th√†nh images v·ªõi parallel processing
    convert_from_path(
        pdf_path,
        dpi=200,
        output_folder=out_dir,
        fmt="png",
        thread_count=thread_count,  # ‚Üê TƒÇNG T·ªêC: Convert pages song song
        use_pdftocairo=True  # ‚Üê TƒÉng t·ªëc: D√πng pdftocairo (nhanh h∆°n pdftoppm)
    )
    
    convert_time = time.time() - convert_start
    logger.info(f"‚úÖ PDF converted to images in {convert_time:.2f} seconds")
    
    # S·ª≠ d·ª•ng pattern t√™n file t·∫°m m√† convert_from_path t·∫°o ra ƒë·ªÉ ƒë·∫£m b·∫£o ƒë√∫ng th·ª© t·ª± trang
    # Th√¥ng th∆∞·ªùng t√™n file l√†: [t√™n file pdf (c√≥ th·ªÉ th√™m bunch random chars)]-1.png, -2.png,...
    temp_image_paths = glob.glob(f"{out_dir}/*.png")

    # ∆Øu ti√™n extract lu√¥n s·ªë trang t·ª´ t√™n file, kh√¥ng d·ª±a v√†o th·ªùi gian modifed (ƒë√¥i khi OS ghi disk kh√¥ng ƒë√∫ng order)
    def extract_page_from_tmpimg(filename):
        # L·∫•y ra s·ªë cu·ªëi tr∆∞·ªõc .png:   ...-1.png, ...-2.png, ...
        basename = os.path.basename(filename)
        match = re.search(r'-(\d+)\.png$', basename)
        return int(match.group(1)) if match else 0
    
    temp_image_paths_sorted = sorted(temp_image_paths, key=extract_page_from_tmpimg)

    # ƒê·ªïi t√™n c√°c file ·∫£nh theo format: t√™n_file_pdf-1.png, t√™n_file_pdf-2.png, ...
    image_paths = []
    for idx, temp_path in enumerate(temp_image_paths_sorted, start=1):
        new_name = f"{pdf_basename}-{idx}.png"
        new_path = os.path.join(out_dir, new_name)
        try:
            os.rename(temp_path, new_path)
            image_paths.append(new_path)
        except Exception as e:
            logger.error(f"‚ùå Failed to rename {temp_path} to {new_path}: {e}")
            image_paths.append(temp_path)
    logger.info(f"üìÑ Found {len(image_paths)} images (renamed to {pdf_basename}-N.png format)")
    return image_paths

# ============================================================================
# OCR Functions
# ============================================================================
def image2text(image_path, model, api, client=None):
    """OCR image -> text with explicit retry logs."""
    if client is None:
        client = OpenAI(base_url=api, api_key="EMPTY", timeout=180.0)

    # Read file once and encode to base64
    with open(image_path, "rb") as f:
        b64 = base64.b64encode(f.read()).decode("utf-8")

    # ƒêo·∫°n n√†y th·ª±c hi·ªán retry (th·ª≠ l·∫°i t·ªëi ƒëa max_attempts l·∫ßn), m·ªói l·∫ßn fail th√¨ backoff v√† log chi ti·∫øt
    max_attempts = 3  # S·ªë l·∫ßn th·ª≠ l·∫°i t·ªëi ƒëa
    for attempt in range(1, max_attempts + 1):  # L·∫∑p qua t·ª´ng l·∫ßn th·ª≠
        try:
            # G·ª≠i request nh·∫≠n di·ªán OCR qua API client
            resp = client.chat.completions.create(
                model=model,  # Ch·ªçn model cho OCR
                messages=[{
                    "role": "user",
                    "content": [
                        {"type": "text", "text": "Extract structured markdown from this page."},  # Y√™u c·∫ßu t·∫°o markdown c√≥ c·∫•u tr√∫c
                        {"type": "image_url", "image_url": {"url": f"data:image/png;base64,{b64}"}}
                    ],
                }],
                temperature=0.0,    # ƒê·∫∑t nhi·ªát th·∫•p nh·∫•t ƒë·ªÉ k·∫øt qu·∫£ ·ªïn ƒë·ªãnh
                max_tokens=4096,    # Gi·ªõi h·∫°n token ƒë·∫ßu ra
                timeout=180.0,      # Timeout cho request
            )
            page_text = resp.choices[0].message.content or ""  # L·∫•y k·∫øt qu·∫£ markdown, fallback sang r·ªóng n·∫øu kh√¥ng c√≥
            logger.info(f"‚úÖ OCR: {os.path.basename(image_path)} -> {len(page_text)} chars (attempt {attempt})")  # Log th√†nh c√¥ng, s·ªë k√Ω t·ª± tr·∫£ v·ªÅ
            return page_text  # Tr·∫£ v·ªÅ n·ªôi dung
        except Exception as e:
            if attempt < max_attempts:
                # N·∫øu ch∆∞a h·∫øt s·ªë l·∫ßn th·ª≠ l·∫°i, log c·∫£nh b√°o k√®m s·ªë l·∫ßn retry v√† l·ªói
                logger.warning(
                    f"‚ö†Ô∏è Retry {attempt}/{max_attempts - 1} for {os.path.basename(image_path)}: {type(e).__name__}: {e}"
                )
                # Th·ª±c hi·ªán "exponential backoff": c√†ng l·ªói nhi·ªÅu c√†ng ch·ªù l√¢u d·∫ßn (delay = 1s, 2s, 4s,...)
                try:
                    time.sleep(1.0 * (2 ** (attempt - 1)))
                except Exception:
                    pass  # N·∫øu sleep c≈©ng l·ªói th√¨ b·ªè qua lu√¥n
            else:
                # N·∫øu h·∫øt s·ªë l·∫ßn th·ª≠ l·∫°i, log l·ªói cu·ªëi c√πng v√† tr·∫£ chu·ªói r·ªóng
                logger.error(f"‚ùå Exhausted retries for {os.path.basename(image_path)}: {type(e).__name__}: {e}")
                return ""

def text2markdown(page_text):
    """Convert HTML to markdown"""
    return markdownify(page_text, heading_style="ATX")

# ============================================================================
# OPTIMIZE 2: OCR Processing Function v·ªõi utils_parallel_batch_size_max_worker.py
# ============================================================================
def process_single_image_ocr(image_path: str, model: str, api: str, **kwargs) -> Optional[str]:
    """
    Process single image: OCR -> Markdown -> Save temp file -> Delete image
    Function n√†y s·∫Ω ƒë∆∞·ª£c d√πng b·ªüi ParallelBatchProcessor
    
    Args:
        image_path: Path to image file
        model: Model name
        api: API endpoint
        **kwargs: Additional arguments (ignored)
        
    Returns:
        markdown text or None if error
    """
    try:
        # Create thread-safe client
        client = OpenAI(base_url=api, api_key="EMPTY", timeout=120.0)
        
        # OCR: image -> text -> markdown
        page_text = image2text(image_path, model, api, client)
        markdown = text2markdown(page_text)
        
        # Check content sau khi OCR th√†nh c√¥ng: n·∫øu tr·ªëng th√¨ set placeholder
        if not markdown.strip():
            markdown = "*[Trang tr·ªëng]*"
            logger.warning(f"‚ö†Ô∏è  Trang tr·ªëng: {os.path.basename(image_path)}")
        
        # CH·ªà t·∫°o file .md khi OCR th√†nh c√¥ng (kh√¥ng c√≥ exception)
        md_temp_path = os.path.splitext(image_path)[0] + ".md"
        with open(md_temp_path, "w", encoding="utf-8") as f:
            f.write(markdown)
        logger.debug(f"üíæ Saved: {os.path.basename(md_temp_path)}")
        
        return markdown
        
    except Exception as e:
        # N·∫øu OCR l·ªói th√¨ KH√îNG t·∫°o file .md
        logger.error(f"‚ùå Error processing {image_path}: {e}")
        return None

# ============================================================================
# Main Pipeline: PDF -> Images (parallel) -> OCR (parallel) -> Markdown
# ============================================================================
def pdf2finalmarkdown(pdf_path, out_dir, model, api, output_md, max_workers=None):
    """
    Convert PDF -> Images (parallel) -> OCR (parallel) -> Markdown
    
    Args:
        pdf_path: Path to PDF file
        out_dir: Output directory for images
        model: Model name
        api: API endpoint
        output_md: Output markdown file path
        max_workers: Max workers for OCR (None = auto-detect)
    """
    total_start = time.time()
    
    logger.info(f"üöÄ Start processing: {pdf_path}")
    
    # Step 1: Convert PDF -> Images (PARALLEL)
    pdf_start = time.time()
    image_paths = pdf2listimages(pdf_path, out_dir, thread_count=PDF_CONVERT_THREADS)
    pdf_time = time.time() - pdf_start
    logger.info(f"‚úÖ PDF converted to images in {pdf_time:.2f} seconds")
    if not image_paths:
        logger.error("No images generated from PDF!")
        return
    
    # Step 2: Process images -> Markdown (PARALLEL)
    # S·ª≠ d·ª•ng process_images_parallel t·ª´ utils_parallel_num_worker.py
    ocr_start = time.time()
    logger.info(f"üöÄ Processing {len(image_paths)} images in parallel...")
    
    result = process_images_parallel(
        list_image_paths=image_paths,
        max_workers=max_workers or OCR_MAX_WORKERS,
        model=model,
        api=api,
        process_func=process_single_image_ocr
    )
    
    ocr_time = time.time() - ocr_start
    
    # Log k·∫øt qu·∫£ x·ª≠ l√Ω
    logger.info(f"‚úÖ OCR completed: {result['total_ok']}/{result['total']} images successful, {result['total_err']} errors")
    logger.info(f"‚è±Ô∏è  OCR processing time: {ocr_time:.2f}s")
    
    # Step 3: Merge markdown t·ª´ c√°c file t·∫°m trong out_dir
    md_files_all = glob.glob(f"{out_dir}/*.md")
    
    if not md_files_all:
        logger.error("No markdown files found in output directory!")
        return
    
    # Check n·∫øu c√≥ trang OCR l·ªói (thi·∫øu file .md) ‚Üí D·ª™NG LU√îN
    if len(md_files_all) < len(image_paths):
        missing_count = len(image_paths) - len(md_files_all)
        logger.error(f"‚ùå C√≥ {missing_count} trang OCR l·ªói (kh√¥ng t·∫°o file .md) - D·ª™NG X·ª¨ L√ù")
        logger.error(f"   T·ªïng s·ªë images: {len(image_paths)} | S·ªë file .md: {len(md_files_all)}")
        
        # T√¨m v√† log c√°c file image kh√¥ng c√≥ file .md t∆∞∆°ng ·ª©ng
        md_basenames = {os.path.splitext(os.path.basename(md_file))[0] for md_file in md_files_all}
        missing_images = []
        for image_path in image_paths:
            image_basename = os.path.splitext(os.path.basename(image_path))[0]
            if image_basename not in md_basenames:
                missing_images.append(os.path.basename(image_path))
        
        if missing_images:
            logger.error(f"   üìã Danh s√°ch c√°c file image OCR l·ªói ({len(missing_images)} files):")
            for img_file in sorted(missing_images):
                logger.error(f"      - {img_file}")
            
            # L∆∞u v√†o file fail.log
            fail_log_path = "fail.log"
            try:
                with open(fail_log_path, "a", encoding="utf-8") as f:
                    f.write(f"\n{'='*80}\n")
                    f.write(f"PDF: {pdf_path}\n")
                    f.write(f"Th·ªùi gian: {time.strftime('%Y-%m-%d %H:%M:%S')}\n")
                    f.write(f"T·ªïng s·ªë images: {len(image_paths)} | S·ªë file .md: {len(md_files_all)} | Thi·∫øu: {missing_count}\n")
                    f.write(f"Danh s√°ch c√°c file image OCR l·ªói ({len(missing_images)} files):\n")
                    for img_file in sorted(missing_images):
                        f.write(f"  - {img_file}\n")
                    f.write(f"{'='*80}\n")
                logger.info(f"üíæ ƒê√£ l∆∞u th√¥ng tin l·ªói v√†o: {fail_log_path}")
            except Exception as e:
                logger.error(f"‚ùå Kh√¥ng th·ªÉ ghi v√†o file {fail_log_path}: {e}")
        
        return
    
    # S·∫Øp x·∫øp file theo s·ªë page (extract t·ª´ t√™n file: xxx-1.md -> 1)
    def extract_page_number(filepath):
        match = re.search(r'-(\d+)\.md$', os.path.basename(filepath))
        return int(match.group(1)) if match else 0
    
    md_files = sorted(md_files_all, key=extract_page_number)
    logger.info(f"üìÑ Found {len(md_files)} markdown files")
    
    # ƒê·ªçc v√† g·ªôp t·∫•t c·∫£ c√°c file markdown
    # L∆∞u √Ω: Ch·ªâ c√≥ file .md khi OCR th√†nh c√¥ng (ƒë√£ x·ª≠ l√Ω trang tr·ªëng ·ªü b∆∞·ªõc OCR)
    md_data = []  # List of (page_num, content)
    for md_file in md_files:
        try:
            with open(md_file, "r", encoding="utf-8") as f:
                content = f.read()
                # Extract s·ªë trang t·ª´ t√™n file (v√≠ d·ª•: xxx-5.md -> 5)
                page_num = extract_page_number(md_file)
                md_data.append((page_num, content))
        except Exception as e:
            logger.error(f"‚ùå Error reading {md_file}: {e}")
    
    if not md_data:
        logger.error("No valid markdown content found!")
        return
    
    # S·∫Øp x·∫øp l·∫°i theo s·ªë trang (ƒë·∫£m b·∫£o th·ª© t·ª± ƒë√∫ng)
    md_data.sort(key=lambda x: x[0])
    
    # G·ªôp v√† l∆∞u file markdown cu·ªëi c√πng (d√πng s·ªë trang t·ª´ t√™n file, kh√¥ng ph·∫£i s·ªë tu·∫ßn t·ª±)
    os.makedirs(os.path.dirname(output_md), exist_ok=True)
    merged = []
    for page_num, content in md_data:
        # S·ªë trang l·∫•y t·ª´ ƒëu√¥i file .png (v√≠ d·ª•: xxx-5.png -> Trang 5)
        merged.append(f"Trang {page_num}\n\n{content}\n\n---")
    merged_text = "\n\n".join(merged).rstrip("-\n")
    with open(output_md, "w", encoding="utf-8") as f:
        f.write(merged_text)
    logger.info(f"üíæ Saved: {output_md} ({len(md_data)} pages)")
   
    # So s√°nh s·ªë trang c·ªßa pdf v√† s·ªë trang c·ªßa markdown
    pdf_pages, md_pages, is_match = compare_page_counts(pdf_path, output_md)
    logger.info(f"PDF pages: {pdf_pages}")
    logger.info(f"Markdown pages: {md_pages}")
    logger.info(f"Match: {is_match}")
    if not is_match:
        logger.error(f"‚ùå S·ªë trang c·ªßa pdf ({pdf_pages}) kh√¥ng b·∫±ng s·ªë trang c·ªßa markdown ({md_pages})")
        # Th√™m ƒë∆∞·ªùng d·∫´n c·ªßa file markdown fail v√† pdf fail v√†o file fail.txt
        with open("fail.txt", "a", encoding="utf-8") as f:
            f.write(f"{pdf_path} -> {output_md}\n")
        return
    
    # X√≥a c√°c file markdown t·∫°m
    for md_file in md_files:
        try:
            os.remove(md_file)
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è  Failed to delete {md_file}: {e}")
    
    total_time = time.time() - total_start
    logger.info(f"\nüéØ TOTAL TIME: {total_time/60:.1f} minutes")
    logger.info(f"   üìÑ PDF conversion: {pdf_time:.2f}s")
    logger.info(f"   üîç OCR processing: {ocr_time:.2f}s")
    logger.info(f"   üìù Merge & save: {(total_time - pdf_time - ocr_time):.2f}s")

if __name__ == "__main__":
    start_time = time.time()
    pdf2finalmarkdown(PDF, OUT_DIR, MODEL, API, OUT_MD, max_workers=OCR_MAX_WORKERS)
    end_time = time.time()
    logger.info(f"‚è±Ô∏è  Total execution time: {end_time - start_time:.2f} seconds")